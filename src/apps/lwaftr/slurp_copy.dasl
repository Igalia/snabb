-- Copy routines optimized for dependency-less slurping -*- lua -*-
--
-- Modern Xeon chips can have up to 74 pending memory loads at once.  The
-- store buffer is smaller, http://www.realworldtech.com/haswell-cpu/5/.

module(..., package.seeall)

local debug = false

local ffi = require("ffi")
local C = ffi.C

local dasm = require("dasm")

|.arch x64
|.actionlist actions

-- Table keeping machine code alive to the GC.
local anchor = {}

-- Utility: assemble code and optionally dump disassembly.
local function assemble (name, prototype, generator)
   local Dst = dasm.new(actions)
   generator(Dst)
   local mcode, size = Dst:build()
   table.insert(anchor, mcode)
   if debug then
      print("mcode dump: "..name)
      dasm.dump(mcode, size)
   end
   return ffi.cast(prototype, mcode)
end

function make_slurping_copy(stride, entry_count, entry_byte_size)
   local bytes = entry_count * entry_byte_size

   local function gen_multi_copy(Dst)
      -- dst in rdi
      -- src in rsi

      | vzeroall

      local tail_size = bytes % 32
      local tail_mask
      if tail_size ~= 0 then
         tail_mask = ffi.new("uint8_t[32]")
         for i=0,tail_size-1 do tail_mask[i]=255 end
         table.insert(anchor, tail_mask)
         | mov rax, tail_mask
         | vmovdqu ymm15, [rax]
      end

      -- Stream in data from up to 8 regions at once.
      while stride > 0 do
         local count = math.min(stride, 8)
         local to_copy = bytes
         while to_copy >= 32 do
            -- Irritatingly there appears to be a limitation on ymm(n)
            -- and Rq(n) for n >= 8!  Get around it by limiting to
            -- ymm0-ymm7 and using memory for the indexes.
            for i = 0, count-1 do
               | mov rax, [rsi + 8*i]
               | vmovdqu ymm(i), [rax]
               | add rax, 32
               | mov [rsi + 8*i], rax
            end
            for i = 0, count-1 do
               | vmovdqu [rdi + i*bytes], ymm(i)
            end
            | add rdi, 32
            to_copy = to_copy - 32
         end

         if to_copy > 0 then
            assert(to_copy / 4 == math.floor(to_copy / 4),
                   '4-byte alignment required')
            for i = 0, count-1 do
               | mov rax, [rsi + 8*i]
               | vpmaskmovd ymm(i), ymm15, [rax]
               | add rax, to_copy
               | mov [rsi + 8*i], rax
            end
            for i = 0, count-1 do
               | vpmaskmovd [rdi + i*bytes], ymm(i), ymm15
            end
            | add rdi, to_copy
            to_copy = 0
         end

         -- Now the dst has been advanced by BYTES.  Increment for the
         -- parallel strides.
         | add rdi, (count-1)*bytes
         -- Increment the src as well.
         | add rsi, count*8
         stride = stride - count
      end
      | vzeroall
      | ret
   end

   return assemble("multi_copy_"..bytes,
                   "void(*)(void*, void*)",
                   gen_multi_copy)
end

function selftest ()
   print("selftest: slurp_copy")
   local src = ffi.new('uint8_t[78]',
                       { 1,
                         2, 2,
                         3, 3, 3,
                         4, 4, 4, 4,
                         5, 5, 5, 5, 5, -- o/~ golden rings o/~
                         6, 6, 6, 6, 6, 6,
                         7, 7, 7, 7, 7, 7, 7,
                         8, 8, 8, 8, 8, 8, 8, 8,
                         9, 9, 9, 9, 9, 9, 9, 9, 9,
                         10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
                         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
                         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12 })
   local dst = ffi.new('uint8_t[100]')

   for size=0,78 do
      local copy = make_slurping_copy(size)
      for offset=0,77-size do
         ffi.C.memset(dst, 0, 100)
         copy(dst, src + offset)
         for i=0,size-1 do assert(dst[i] == src[offset+i]) end
         for i=size,99 do assert(dst[i] == 0) end
      end
   end

   print("selftest: ok")
end
