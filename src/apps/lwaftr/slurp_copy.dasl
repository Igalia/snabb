-- Copy routines optimized for dependency-less slurping -*- lua -*-
--
-- Modern Xeon chips can have up to 74 pending memory loads at once.  The
-- store buffer is smaller, http://www.realworldtech.com/haswell-cpu/5/.

module(..., package.seeall)

local debug = false

local ffi = require("ffi")
local C = ffi.C

local dasm = require("dasm")

|.arch x64
|.actionlist actions

-- Table keeping machine code alive to the GC.
local anchor = {}

-- Utility: assemble code and optionally dump disassembly.
local function assemble (name, prototype, generator)
   local Dst = dasm.new(actions)
   generator(Dst)
   local mcode, size = Dst:build()
   table.insert(anchor, mcode)
   if debug then
      print("mcode dump: "..name)
      dasm.dump(mcode, size)
   end
   return ffi.cast(prototype, mcode)
end

function make_slurping_copy(stride, entry_count, entry_byte_size)
   local bytes = entry_count * entry_byte_size

   local function load_indices(Dst, count)
      -- Load indices, convert to pointers.
      for i = 0, count-1 do
         | xor eax, eax
         | mov eax, [rdx]
         -- mul outputs to rdx:rax; save rdx.
         | mov rcx, rdx
         | mov rdx, entry_byte_size
         | mul rdx
         | mov rdx, rcx
         | add rdx, 4
         -- r8 through r11 are temporaries.
         -- dynasm refuses to assemble Rq(n) if n is >= 8.  FML.
         -- | lea Rq(8+i), [rsi + rax]
         if i == 0 then
            | lea r8, [rsi+rax]
         elseif i == 1 then
            | lea r9, [rsi+rax]
         elseif i == 2 then
            | lea r10, [rsi+rax]
         elseif i == 3 then
            | lea r11, [rsi+rax]
         else error(i) end
      end
   end

   local function gen_aligned_multi_copy(Dst)
      -- dst in rdi
      -- src in rsi
      -- indices in rdx

      | vzeroall

      -- Fetch up to 4 keys at once; what the hell.
      while stride > 0 do
         local count = math.min(stride, 4)
         load_indices(Dst, count)
         local to_copy = bytes
         while to_copy > 0 do
            -- Irritatingly there appears to be a limitation on ymm(n)
            -- for n >= 8!
            local step = math.min(to_copy/32, 2)
            for j = 0, step-1 do
               for i = 0, count-1 do
                  -- | vmovdqu ymm(i), [Rq(8+i)]
                  -- | add Rq(8+i), 32
                  if i == 0 then
                     | vmovdqu ymm(j*count+i), [r8]
                     | add r8,32
                  elseif i == 1 then
                     | vmovdqu ymm(j*count+i), [r9]
                     | add r9,32
                  elseif i == 2 then
                     | vmovdqu ymm(j*count+i), [r10]
                     | add r10,32
                  elseif i == 3 then
                     | vmovdqu ymm(j*count+i), [r11]
                     | add r11,32
                  end
               end
            end
            for j = 0, step-1 do
               for i = 0, count-1 do
                  | vmovdqu [rdi + i*bytes], ymm(j*count+i)
               end
               | add rdi, 32
            end
            to_copy = to_copy - 32*step
         end

         -- Now rdi has been advanced by BYTES.  Increment for the
         -- parallel strides.
         | add rdi, (count-1)*bytes
         
         stride = stride - count
      end
      | vzeroall
      | ret
   end

   local function gen_unaligned_multi_copy(Dst)
      -- dst in rdi
      -- src in rsi
      -- indices in rdx

      | vzeroall

      local tail_size = bytes % 32
      local tail_mask
      if tail_size ~= 0 then
         tail_mask = ffi.new("uint8_t[32]")
         for i=0,tail_size-1 do tail_mask[i]=255 end
         table.insert(anchor, tail_mask)
         | mov rax, tail_mask
         | vmovdqu ymm15, [rax]
      end

      -- Fetch up to 4 keys at once; what the hell.
      while stride > 0 do
         -- Load indices, convert to pointers.
         for i = 0, math.min(stride, 4)-1 do
            | xor eax, eax
            | mov eax, [rdx]
            -- mul outputs to rdx:rax; save rdx.
            | mov rcx, rdx
            | mov rdx, entry_byte_size
            | mul rdx
            | mov rdx, rcx
            | add rdx, 4
            -- r8 through r11 are temporaries.
            -- dynasm refuses to assemble Rq(n) if n is >= 8.  FML.
            -- | lea Rq(8+i), [rsi + rax]
            if i == 0 then
               | lea r8, [rsi+rax]
            elseif i == 1 then
               | lea r9, [rsi+rax]
            elseif i == 2 then
               | lea r10, [rsi+rax]
            elseif i == 3 then
               | lea r11, [rsi+rax]
            else error(i) end
         end
         local to_copy = bytes
         while to_copy >= 32 do            
            for i = 0, math.min(stride, 4)-1 do
               -- | vmovdqu ymm(i), [Rq(8+i)]
               -- | add Rq(8+i), 32
               if i == 0 then
                  | vmovdqu ymm(i), [r8]
                  | add r8,32
               elseif i == 1 then
                  | vmovdqu ymm(i), [r9]
                  | add r9,32
               elseif i == 2 then
                  | vmovdqu ymm(i), [r10]
                  | add r10,32
               elseif i == 3 then
                  | vmovdqu ymm(i), [r11]
                  | add r11,32
               end
            end
            for i = 0, math.min(stride, 4)-1 do
               | vmovdqu [rdi + i*bytes], ymm(i)
            end
            | add rdi, 32
            to_copy = to_copy - 32
         end
         if to_copy > 0 then
            assert(to_copy / 4 == math.floor(to_copy / 4),
                   '4-byte alignment required')
            for i = 0, math.min(stride, 4)-1 do
               -- | vpmaskmovd ymm(i), ymm15, [Rq(8+i)]
               -- | add Rq(8+i),to_copy
               if i == 0 then
                  | vpmaskmovd ymm(i), ymm15, [r8]
                  | add r8,to_copy
               elseif i == 1 then
                  | vpmaskmovd ymm(i), ymm15, [r9]
                  | add r9,to_copy
               elseif i == 2 then
                  | vpmaskmovd ymm(i), ymm15, [r10]
                  | add r10,to_copy
               elseif i == 3 then
                  | vpmaskmovd ymm(i), ymm15, [r11]
                  | add r11,to_copy
               end
            end
            for i = 0, math.min(stride, 4)-1 do
               | vpmaskmovd [rdi + i*bytes], ymm(i), ymm15
            end
            | add rdi, to_copy
         end

         -- Now rdi has been advanced by BYTES.  Increment for the
         -- parallel strides.
         | add rdi, (math.min(stride,4)-1)*bytes
         
         stride = stride - 4
      end
      | vzeroall
      | ret
   end

   if entry_byte_size % 32 == 0 then
      return assemble("aligned_multi_copy_"..bytes,
                      "void(*)(void*, void*, uint32_t*)",
                      gen_aligned_multi_copy)
   else
      return assemble("unaligned_multi_copy_"..bytes,
                      "void(*)(void*, void*, uint32_t*)",
                      gen_unaligned_multi_copy)
   end
end

function selftest ()
   print("selftest: slurp_copy")
   local src = ffi.new('uint8_t[78]',
                       { 1,
                         2, 2,
                         3, 3, 3,
                         4, 4, 4, 4,
                         5, 5, 5, 5, 5, -- o/~ golden rings o/~
                         6, 6, 6, 6, 6, 6,
                         7, 7, 7, 7, 7, 7, 7,
                         8, 8, 8, 8, 8, 8, 8, 8,
                         9, 9, 9, 9, 9, 9, 9, 9, 9,
                         10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
                         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
                         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12 })
   local dst = ffi.new('uint8_t[100]')

   for size=0,78 do
      local copy = make_slurping_copy(size)
      for offset=0,77-size do
         ffi.C.memset(dst, 0, 100)
         copy(dst, src + offset)
         for i=0,size-1 do assert(dst[i] == src[offset+i]) end
         for i=size,99 do assert(dst[i] == 0) end
      end
   end

   print("selftest: ok")
end
